{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# PyTorch로 RNN, LSTM 구현하기\n","- 출처 : https://justkode.kr/deep-learning/pytorch-rnn\n","\n","\n","- PyTorch에서 제공하는 RNN과 관련 API를 이용해 손쉽게 RNN 네트워크를 구축 할 수 있음\n"],"metadata":{"id":"Xj7yvVePDiwW"}},{"cell_type":"markdown","source":["### 앞의 두 단어를 보고, 뒤에 나올 단어를 예측하는 모델 작성"],"metadata":{"id":"QOsnacyAD9M6"}},{"cell_type":"code","source":["# 필요한 모듈 import 및 학습 데이터 입력\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","sentences = [\"i like dog\", \"i love coffee\", \"i hate milk\", \"you like cat\", \"you love milk\", \"you hate coffee\"]\n","dtype = torch.float"],"metadata":{"id":"gSy-ch_aECTb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 전처리\n","word_list = list(set(\" \".join(sentences).split()))\n","word_dict = {w: i for i, w in enumerate(word_list)}\n","number_dict = {i: w for i, w in enumerate(word_list)}\n","n_class = len(word_dict)"],"metadata":{"id":"lB4Pd3EuFciR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\" \".join(sentences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"6Er1iPZKOXNS","executionInfo":{"status":"ok","timestamp":1681533836237,"user_tz":-540,"elapsed":374,"user":{"displayName":"hwi","userId":"07964455913579077797"}},"outputId":"d7656710-e4bb-46af-a3e5-1b883ae92bbb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'i like dog i love coffee i hate milk you like cat you love milk you hate coffee'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["word_list"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-BJ0MbRzNuIK","executionInfo":{"status":"ok","timestamp":1681533785014,"user_tz":-540,"elapsed":331,"user":{"displayName":"hwi","userId":"07964455913579077797"}},"outputId":"3ea6e13b-f250-4ea0-9dbf-f45c5fbffdb2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['cat', 'milk', 'like', 'i', 'dog', 'you', 'love', 'hate', 'coffee']"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["number_dict"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n7caF6SMNd4E","executionInfo":{"status":"ok","timestamp":1681533787656,"user_tz":-540,"elapsed":333,"user":{"displayName":"hwi","userId":"07964455913579077797"}},"outputId":"e1bd22f9-6c31-43c7-99b2-04cb2fb51735"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'cat',\n"," 1: 'milk',\n"," 2: 'like',\n"," 3: 'i',\n"," 4: 'dog',\n"," 5: 'you',\n"," 6: 'love',\n"," 7: 'hate',\n"," 8: 'coffee'}"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["n_class"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AMaM-a5eQrKa","executionInfo":{"status":"ok","timestamp":1681534405086,"user_tz":-540,"elapsed":5,"user":{"displayName":"hwi","userId":"07964455913579077797"}},"outputId":"e42211bc-09d6-4e88-98de-dd8b624aa6b7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# RNN hyper-parameter 정의\n","\n","batch_size = len(sentences)\n","n_step = 2  # 학습 하려고 하는 어절의 길이 - 1\n","n_hidden = 5  # 은닉층 사이즈"],"metadata":{"id":"8w97rZXeHXFt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습을 위한 batch 구성 함수 정의\n","def make_batch(sentences):\n","  input_batch = []\n","  target_batch = []\n","\n","  for sen in sentences:\n","    word = sen.split() # [\"i\", \"like\", \"dog\"]\n","    input = [word_dict[n] for n in word[:-1]] # [\"i\", \"like\"]\n","    target = word_dict[word[-1]] # [\"dog\"]\n","\n","    '''\n","      print(\"word\", word) # word ['i', 'like', 'dog']\n","      print(\"input\", input) # input [3, 2]\n","      print(\"target\", target) target 4\n","    '''\n","    input_batch.append(np.eye(n_class)[input])  # One-Hot Encoding\n","    target_batch.append(target)\n","  \n","  return input_batch, target_batch"],"metadata":{"id":"pmWW_wwiHXsW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Batch 생성 및 Tensor 화\n","input_batch, target_batch = make_batch(sentences)\n","print(input_batch[0], target_batch)\n","input_batch = torch.tensor(input_batch, dtype=torch.float32, requires_grad=True)\n","target_batch = torch.tensor(target_batch, dtype=torch.int64)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TIk4HkGxHfvQ","outputId":"1c7099f1-10c7-463f-85f0-17cc1a4d5ebc","executionInfo":{"status":"ok","timestamp":1681535158772,"user_tz":-540,"elapsed":6,"user":{"displayName":"hwi","userId":"07964455913579077797"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["word ['i', 'like', 'dog']\n","input [3, 2]\n","target 4\n","word ['i', 'love', 'coffee']\n","input [3, 6]\n","target 8\n","word ['i', 'hate', 'milk']\n","input [3, 7]\n","target 1\n","word ['you', 'like', 'cat']\n","input [5, 2]\n","target 0\n","word ['you', 'love', 'milk']\n","input [5, 6]\n","target 1\n","word ['you', 'hate', 'coffee']\n","input [5, 7]\n","target 8\n","[[0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0.]] [4, 8, 1, 0, 1, 8]\n"]}]},{"cell_type":"code","source":["# 모델 정의\n","class TextRNN(nn.Module):\n","  def __init__(self):\n","    super(TextRNN, self).__init__()\n","\n","    self.rnn = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.3)\n","    self.W = nn.Parameter(torch.randn([n_hidden, n_class]).type(dtype)) # 자동미분이 되는(requires_grad=True) Tensor를 생성\n","    self.b = nn.Parameter(torch.randn([n_class]).type(dtype))\n","    self.Softmax = nn.Softmax(dim=1)\n","\n","  def forward(self, hidden, X):\n","    X = X.transpose(0, 1)\n","    outputs, hidden = self.rnn(X, hidden) # nn.RNN forward 함수 (input: 입력값 X, initial hidden state)\n","    outputs = outputs[-1]  # 최종 예측 Hidden Layer\n","    model = torch.mm(outputs, self.W) + self.b  # 최종 예측 최종 출력 층\n","    return model\n","\t"],"metadata":{"id":"Ev06J-y6IJZc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CsTMNax6Dhmw","outputId":"da4f2c31-fc79-4405-c038-22999bf66615","executionInfo":{"status":"ok","timestamp":1681535688774,"user_tz":-540,"elapsed":820,"user":{"displayName":"hwi","userId":"07964455913579077797"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0100 cost = 0.231202\n","Epoch: 0200 cost = 0.036785\n","Epoch: 0300 cost = 0.017297\n","Epoch: 0400 cost = 0.010481\n","Epoch: 0500 cost = 0.007127\n"]}],"source":["# 모델 학습\n","model = TextRNN()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n","for epoch in range(500):\n","  # nn.RNN에 필요한 hidden state 초기값 정의\n","  hidden = torch.zeros(1, batch_size, n_hidden, requires_grad=True)\n","  output = model(hidden, input_batch)\n","  loss = criterion(output, target_batch)\n","\n","  if (epoch + 1) % 100 == 0:\n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n","  \n","  optimizer.zero_grad()\n","  loss.backward()\n","  optimizer.step()\n"]},{"cell_type":"code","source":["# Test\n","input = [sen.split()[:2] for sen in sentences]\n","hidden = torch.zeros(1, batch_size, n_hidden, requires_grad=True)\n","\n","#print(model(hidden, input_batch).data)\n","#print(model(hidden, input_batch).data.max(1, keepdim=True))\n","\n","predict = model(hidden, input_batch).data.max(1, keepdim=True)[1]\n","\n","print(predict.squeeze())\n","print(number_dict)\n","\n","print([sen.split()[:2] for sen in sentences], '->', [number_dict[n.item()] for n in predict.squeeze()])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4lmFw8j0JW-K","outputId":"f3235adb-87bc-4ab8-8e53-08943671b341","executionInfo":{"status":"ok","timestamp":1681536463570,"user_tz":-540,"elapsed":351,"user":{"displayName":"hwi","userId":"07964455913579077797"}}},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.return_types.max(\n","values=tensor([[5.4243],\n","        [6.9860],\n","        [7.5515],\n","        [7.3983],\n","        [8.3421],\n","        [8.3252]]),\n","indices=tensor([[4],\n","        [8],\n","        [1],\n","        [0],\n","        [1],\n","        [8]]))\n","tensor([4, 8, 1, 0, 1, 8])\n","{0: 'cat', 1: 'milk', 2: 'like', 3: 'i', 4: 'dog', 5: 'you', 6: 'love', 7: 'hate', 8: 'coffee'}\n","[['i', 'like'], ['i', 'love'], ['i', 'hate'], ['you', 'like'], ['you', 'love'], ['you', 'hate']] -> ['dog', 'coffee', 'milk', 'cat', 'milk', 'coffee']\n"]}]},{"cell_type":"code","source":["# TextLSTM 모델 정의\n","\n","class TextLSTM(nn.Module):\n","  def __init__(self):\n","    super(TextLSTM, self).__init__()\n","\n","    self.lstm = nn.LSTM(input_size=n_class, hidden_size=n_hidden, dropout=0.3)\n","    self.W = nn.Parameter(torch.randn([n_hidden, n_class]).type(dtype))\n","    self.b = nn.Parameter(torch.randn([n_class]).type(dtype))\n","    self.Softmax = nn.Softmax(dim=1)\n","\n","  def forward(self, hidden_and_cell, X):\n","    X = X.transpose(0, 1)\n","    outputs, hidden = self.lstm(X, hidden_and_cell) # hidden_and_cell : (hidden state 초기값, cell state 초기값)\n","    outputs = outputs[-1]  # 최종 예측 Hidden Layer\n","    model = torch.mm(outputs, self.W) + self.b  # 최종 예측 최종 출력 층\n","    return model\n"],"metadata":{"id":"v-r_NInIMeb3","executionInfo":{"status":"ok","timestamp":1681536604057,"user_tz":-540,"elapsed":338,"user":{"displayName":"hwi","userId":"07964455913579077797"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["\t# 모델 학습\n","model = TextLSTM()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n","for epoch in range(500):\n","  hidden = torch.zeros(1, batch_size, n_hidden, requires_grad=True)\n","  cell = torch.zeros(1, batch_size, n_hidden, requires_grad=True)\n","  output = model((hidden, cell), input_batch)\n","  loss = criterion(output, target_batch)\n","\n","  if (epoch + 1) % 100 == 0:\n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n","  \n","  optimizer.zero_grad()\n","  loss.backward()\n","  optimizer.step()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FFn4FY0lNAGT","outputId":"d2a47066-36f9-49e4-c9bd-437699ce0b6b","executionInfo":{"status":"ok","timestamp":1681536607725,"user_tz":-540,"elapsed":1062,"user":{"displayName":"hwi","userId":"07964455913579077797"}}},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0100 cost = 0.604908\n","Epoch: 0200 cost = 0.159963\n","Epoch: 0300 cost = 0.046787\n","Epoch: 0400 cost = 0.025024\n","Epoch: 0500 cost = 0.016055\n"]}]},{"cell_type":"code","source":["input = [sen.split()[:2] for sen in sentences]\n","\n","hidden = torch.zeros(1, batch_size, n_hidden, requires_grad=True)\n","cell = torch.zeros(1, batch_size, n_hidden, requires_grad=True)\n","predict = model((hidden, cell), input_batch).data.max(1, keepdim=True)[1]\n","print([sen.split()[:2] for sen in sentences], '->', [number_dict[n.item()] for n in predict.squeeze()])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GMtDRRJJNBF_","outputId":"7b615952-32fe-475c-fa31-0113334ca958","executionInfo":{"status":"ok","timestamp":1681536616723,"user_tz":-540,"elapsed":518,"user":{"displayName":"hwi","userId":"07964455913579077797"}}},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["[['i', 'like'], ['i', 'love'], ['i', 'hate'], ['you', 'like'], ['you', 'love'], ['you', 'hate']] -> ['dog', 'coffee', 'milk', 'cat', 'milk', 'coffee']\n"]}]}]}