{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "251709a4",
   "metadata": {},
   "source": [
    "# RecSys Model Ensemble (미완성)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35271bbd",
   "metadata": {},
   "source": [
    "##### Using Prediction Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21662d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "def ensemble_recommendations(folder_path, final_K, scaling_method='minmax', weights=None):\n",
    "    # 주어진 폴더에서 .csv 화일 목록 가져오기\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    \n",
    "    # 각 화일을 데이터프레임으로 읽어오기\n",
    "    dfs = [pd.read_csv(os.path.join(folder_path, csv_file)) for csv_file in csv_files]\n",
    "    \n",
    "    # 가중치가 주어지지 않은 경우, 모든 알고리즘에 동일한 가중치 (1) 부여\n",
    "    if weights is None:\n",
    "        weights = [1] * len(dfs)\n",
    "    \n",
    "    # 스케일러 초기화\n",
    "    if scaling_method == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif scaling_method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    \n",
    "    # 모든 데이터프레임들을 하나로 합치기 전에 각 데이터프레임마다 prediction 값을 정규화\n",
    "    normalized_dfs = []\n",
    "    for i, df in enumerate(dfs):\n",
    "        df['prediction'] = scaler.fit_transform(df['prediction'].values.reshape(-1, 1)).flatten()\n",
    "        # 가중치 적용\n",
    "        df['prediction'] *= weights[i]\n",
    "        normalized_dfs.append(df)\n",
    "\n",
    "    # 정규화 및 가중치 적용된 데이터프레임들을 하나로 합치기\n",
    "    combined_df = pd.concat(normalized_dfs, ignore_index=True)\n",
    "    \n",
    "    # resume_seq와 recruitment_seq를 기준으로 prediction 값을 그룹화하고 평균 계산\n",
    "    averaged_predictions = combined_df.groupby(['resume_seq', 'recruitment_seq']).prediction.mean().reset_index()\n",
    "    \n",
    "    # 평균 점수를 기준으로 내림차순 정렬\n",
    "    top_k = (\n",
    "        averaged_predictions\n",
    "        .groupby('resume_seq')\n",
    "        .apply(lambda x: x.nlargest(final_K, 'prediction'))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7ab7e3",
   "metadata": {},
   "source": [
    "##### Using Ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08091fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "def ensemble_recommendations(folder_path, final_K, scaling_method='minmax', weights=None):\n",
    "    # 주어진 폴더에서 .csv 화일 목록 가져오기\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    \n",
    "    # 각 화일을 데이터프레임으로 읽어오기\n",
    "    dfs = [pd.read_csv(os.path.join(folder_path, csv_file)) for csv_file in csv_files]\n",
    "    \n",
    "    # 가중치가 주어지지 않은 경우, 모든 알고리즘에 동일한 가중치 (1) 부여\n",
    "    if weights is None:\n",
    "        weights = [1] * len(dfs)\n",
    "    \n",
    "    # 모든 데이터프레임에서 prediction 값을 기반으로 순위를 계산하고 저장\n",
    "    ranked_dfs = []\n",
    "    for i, df in enumerate(dfs):\n",
    "        df['prediction'] = df.groupby('resume_seq')['prediction'].rank(ascending=False, method='first')\n",
    "        # 가중치 적용\n",
    "        df['prediction'] *= weights[i]\n",
    "        ranked_dfs.append(df)\n",
    "\n",
    "    # 순위 및 가중치 적용된 데이터프레임들을 하나로 합치기\n",
    "    combined_df = pd.concat(ranked_dfs, ignore_index=True)\n",
    "    \n",
    "    # resume_seq와 recruitment_seq를 기준으로 prediction 값을 그룹화하고 평균 계산\n",
    "    averaged_predictions = combined_df.groupby(['resume_seq', 'recruitment_seq']).prediction.mean().reset_index()\n",
    "    \n",
    "    # 평균 순위를 기준으로 오름차순 정렬 (낮은 순위가 높은 선호도를 의미)\n",
    "    top_k = (\n",
    "        averaged_predictions\n",
    "        .groupby('resume_seq')\n",
    "        .apply(lambda x: x.nsmallest(final_K, 'prediction'))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef5cd3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 호출\n",
    "recommendations = ensemble_recommendations('./preds', final_K=5)\n",
    "\n",
    "# 결과 저장\n",
    "recommendations[['resume_seq', 'recruitment_seq']].to_csv('submission_ensemble.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d4728a",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch2.0,Tensorflow2.12 (kaggle v135 23.07/ Python Conda 3.10,CUDA 11.8) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
